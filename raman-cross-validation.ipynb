{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validated MSE: 62.5645751953125 ± 42.22898864746094\n",
      "Cross-Validated MAE: 5.701310634613037 ± 2.4557509422302246\n",
      "Cross-Validated R^2: 0.9307197451591491 ± 0.04716478768497243\n",
      "R^2 on test set: 0.9314345549773472\n",
      "File name: test-file-20.dpt\n",
      "Predicted Percentage: 22.097980499267578\n",
      "MSE for test-file-20.dpt: 4.401522175307036\n",
      "MAE for test-file-20.dpt: 2.097980499267578\n",
      "File name: test-file-66.dpt\n",
      "Predicted Percentage: 59.00474166870117\n",
      "MSE for test-file-66.dpt: 57.68794911916417\n",
      "MAE for test-file-66.dpt: 7.595258331298822\n",
      "File name: test-file-33.dpt\n",
      "Predicted Percentage: 40.44773864746094\n",
      "MSE for test-file-33.dpt: 51.09016777240675\n",
      "MAE for test-file-33.dpt: 7.14773864746094\n",
      "File name: test-file-50.dpt\n",
      "Predicted Percentage: 46.09074783325195\n",
      "MSE for test-file-50.dpt: 15.2822525032243\n",
      "MAE for test-file-50.dpt: 3.909252166748047\n",
      "File name: test-file-51.dpt\n",
      "Predicted Percentage: 52.104339599609375\n",
      "MSE for test-file-51.dpt: 4.428245150484145\n",
      "MAE for test-file-51.dpt: 2.104339599609375\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Path to the folder containing the .csv files\n",
    "folder_path = 'data/csv-clean-renamed'  # Adjust the path to the directory\n",
    "\n",
    "# Read the data\n",
    "wavenumbers = []\n",
    "data = []\n",
    "labels = []\n",
    "\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('.csv'):\n",
    "        filepath = os.path.join(folder_path, filename)\n",
    "        spectrum = pd.read_csv(filepath, delimiter=',', header=None)\n",
    "        wavenumbers.append(spectrum[0].values)  # Assuming first column is wavelength\n",
    "        data.append(spectrum[1].values)  # Assuming second column is intensity\n",
    "        # Extract concentration from filename\n",
    "        concentration = float(filename.split('_')[0].replace('conc-', ''))\n",
    "        labels.append(concentration)\n",
    "\n",
    "wavenumbers = np.array(wavenumbers)\n",
    "data = np.array(data)\n",
    "labels = np.array(labels)\n",
    "\n",
    "max_wavenumber = 1800\n",
    "index_max_1500 = np.where(wavenumbers[0] <= max_wavenumber)[0][-1]\n",
    "data_trimmed = data[:, :index_max_1500 + 1]\n",
    "data_normalized = np.array([spectrum / np.sum(spectrum) for spectrum in data_trimmed])\n",
    "\n",
    "# Standard scaling\n",
    "scaler = StandardScaler()\n",
    "data_normalized = scaler.fit_transform(data_normalized)\n",
    "\n",
    "X = data_normalized\n",
    "y = labels\n",
    "\n",
    "# Define PyTorch dataset and dataloader\n",
    "class SpectraDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "# Define the neural network model\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 32)\n",
    "        self.fc4 = nn.Linear(32, 1)\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "# Training function\n",
    "def train_model(model, criterion, optimizer, dataloader, num_epochs=200):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        for inputs, labels in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model(model, dataloader):\n",
    "    model.eval()\n",
    "    predictions, true_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            outputs = model(inputs)\n",
    "            predictions.append(outputs.numpy())\n",
    "            true_labels.append(labels.numpy())\n",
    "    return np.vstack(predictions), np.vstack(true_labels)\n",
    "\n",
    "# Cross-validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "mse_scores, mae_scores, r2_scores = [], [], []\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    train_dataset = SpectraDataset(X_train, y_train)\n",
    "    test_dataset = SpectraDataset(X_test, y_test)\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    model = Net(input_size=X_train.shape[1])\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    train_model(model, criterion, optimizer, train_loader, num_epochs=250)\n",
    "    predictions, true_labels = evaluate_model(model, test_loader)\n",
    "\n",
    "    mse = mean_squared_error(true_labels, predictions)\n",
    "    mae = mean_absolute_error(true_labels, predictions)\n",
    "    r2 = r2_score(true_labels, predictions)\n",
    "\n",
    "    mse_scores.append(mse)\n",
    "    mae_scores.append(mae)\n",
    "    r2_scores.append(r2)\n",
    "\n",
    "print(f\"Cross-Validated MSE: {np.mean(mse_scores)} ± {np.std(mse_scores)}\")\n",
    "print(f\"Cross-Validated MAE: {np.mean(mae_scores)} ± {np.std(mae_scores)}\")\n",
    "print(f\"Cross-Validated R^2: {np.mean(r2_scores)} ± {np.std(r2_scores)}\")\n",
    "\n",
    "# Feature importance\n",
    "weights = model.fc1.weight.data.numpy()\n",
    "feature_importance = np.sum(np.abs(weights), axis=0)\n",
    "indices = np.argsort(feature_importance)[::-1]\n",
    "\n",
    "# print(\"Feature ranking:\")\n",
    "# for i in range(len(feature_importance)):\n",
    "#     print(f\"Feature {indices[i]} (Wavenumber {wavenumbers[0][indices[i]]}): {feature_importance[indices[i]]}\")\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "# Final evaluation on a separate test set\n",
    "model.eval()\n",
    "predictions = model(torch.tensor(X_test, dtype=torch.float32)).detach().numpy()\n",
    "r2 = r2_score(y_test, predictions)\n",
    "print(f\"R^2 on test set: {r2}\")\n",
    "\n",
    "# Dictionary containing true values for the unknown samples\n",
    "true_values = {\n",
    "    'test-file-20.dpt': 20.0,  # Replace with actual values\n",
    "    'test-file-66.dpt': 66.6,\n",
    "    'test-file-33.dpt': 33.3,\n",
    "    'test-file-50.dpt': 50.0,\n",
    "    'test-file-51.dpt': 50.0\n",
    "}\n",
    "\n",
    "# Prediction for an unknown sample\n",
    "def predict_concentration_for_unknown_samples(unknown_sample_filename):\n",
    "    print(f'File name: {unknown_sample_filename}')\n",
    "    unknown_sample_filepath = os.path.join('C:\\\\Users\\\\baubl\\\\OneDrive\\\\Stalinis kompiuteris\\\\Nuova cartella\\\\test', unknown_sample_filename)\n",
    "    unknown_spectrum = pd.read_csv(unknown_sample_filepath, delimiter=',', header=None)\n",
    "    index_max_1800 = np.where(unknown_spectrum[0] <= max_wavenumber)[0][-1]\n",
    "    data_trimmed_unknown = unknown_spectrum[1].values[:index_max_1800 + 1]\n",
    "    normalized_unknown_spectrum = data_trimmed_unknown / np.sum(data_trimmed_unknown)\n",
    "    normalized_unknown_spectrum = scaler.transform([normalized_unknown_spectrum])  # Apply same scaling\n",
    "    normalized_unknown_spectrum = torch.tensor(normalized_unknown_spectrum, dtype=torch.float32)\n",
    "\n",
    "    predicted_percentage = model(normalized_unknown_spectrum).item()\n",
    "    print(f'Predicted Percentage: {predicted_percentage}')\n",
    "    \n",
    "    # Calculate MSE and MAE for the prediction\n",
    "    true_value = true_values[unknown_sample_filename]\n",
    "    mse_value = mean_squared_error([true_value], [predicted_percentage])\n",
    "    mae_value = mean_absolute_error([true_value], [predicted_percentage])\n",
    "    print(f'MSE for {unknown_sample_filename}: {mse_value}')\n",
    "    print(f'MAE for {unknown_sample_filename}: {mae_value}')\n",
    "\n",
    "# Predict and print MSE and MAE values for each file\n",
    "predict_concentration_for_unknown_samples('test-file-20.dpt')\n",
    "predict_concentration_for_unknown_samples('test-file-66.dpt')\n",
    "predict_concentration_for_unknown_samples('test-file-33.dpt')\n",
    "predict_concentration_for_unknown_samples('test-file-50.dpt')\n",
    "predict_concentration_for_unknown_samples('test-file-51.dpt')\n",
    "\n",
    "# Feature importance\n",
    "weights = model.fc1.weight.data.numpy()\n",
    "feature_importance = np.sum(np.abs(weights), axis=0)\n",
    "indices = np.argsort(feature_importance)[::-1]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validated MSE: 40.792747497558594 ± 21.77153205871582\n",
      "Cross-Validated MAE: 4.825381278991699 ± 1.829215407371521\n",
      "Cross-Validated R^2: 0.955007815361023 ± 0.024078592290517502\n",
      "Detected Peaks: []\n",
      "No peaks detected or processed.\n",
      "R^2 on test set: 0.935496237271733\n",
      "File name: test-file-20.dpt\n",
      "Predicted Percentage: 22.205801010131836\n",
      "MSE for test-file-20.dpt: 4.865558096298628\n",
      "MAE for test-file-20.dpt: 2.205801010131836\n",
      "File name: test-file-66.dpt\n",
      "Predicted Percentage: 67.46195220947266\n",
      "MSE for test-file-66.dpt: 0.7429616114148037\n",
      "MAE for test-file-66.dpt: 0.8619522094726619\n",
      "File name: test-file-33.dpt\n",
      "Predicted Percentage: 46.634307861328125\n",
      "MSE for test-file-33.dpt: 177.8037661406771\n",
      "MAE for test-file-33.dpt: 13.334307861328128\n",
      "File name: test-file-50.dpt\n",
      "Predicted Percentage: 50.88127136230469\n",
      "MSE for test-file-50.dpt: 0.7766392140183598\n",
      "MAE for test-file-50.dpt: 0.8812713623046875\n",
      "File name: test-file-51.dpt\n",
      "Predicted Percentage: 56.36664581298828\n",
      "MSE for test-file-51.dpt: 40.53417890804121\n",
      "MAE for test-file-51.dpt: 6.366645812988281\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import find_peaks\n",
    "\n",
    "# Path to the folder containing the .csv files\n",
    "folder_path = 'data/csv-clean-renamed'  # Adjust the path to the directory\n",
    "\n",
    "# Read the data\n",
    "wavenumbers = []\n",
    "data = []\n",
    "labels = []\n",
    "\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('.csv'):\n",
    "        filepath = os.path.join(folder_path, filename)\n",
    "        spectrum = pd.read_csv(filepath, delimiter=',', header=None)\n",
    "        wavenumbers.append(spectrum[0].values)  # Assuming first column is wavelength\n",
    "        data.append(spectrum[1].values)  # Assuming second column is intensity\n",
    "        # Extract concentration from filename\n",
    "        concentration = float(filename.split('_')[0].replace('conc-', ''))\n",
    "        labels.append(concentration)\n",
    "\n",
    "wavenumbers = np.array(wavenumbers)\n",
    "data = np.array(data)\n",
    "labels = np.array(labels)\n",
    "\n",
    "max_wavenumber = 1800\n",
    "index_max_1500 = np.where(wavenumbers[0] <= max_wavenumber)[0][-1]\n",
    "data_trimmed = data[:, :index_max_1500 + 1]\n",
    "data_normalized = np.array([spectrum / np.sum(spectrum) for spectrum in data_trimmed])\n",
    "\n",
    "# Standard scaling\n",
    "scaler = StandardScaler()\n",
    "data_normalized = scaler.fit_transform(data_normalized)\n",
    "\n",
    "X = data_normalized\n",
    "y = labels\n",
    "\n",
    "# Define PyTorch dataset and dataloader\n",
    "class SpectraDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "# Define the neural network model\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 32)\n",
    "        self.fc4 = nn.Linear(32, 1)\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "# Training function\n",
    "def train_model(model, criterion, optimizer, dataloader, num_epochs=200):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        for inputs, labels in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model(model, dataloader):\n",
    "    model.eval()\n",
    "    predictions, true_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            outputs = model(inputs)\n",
    "            predictions.append(outputs.numpy())\n",
    "            true_labels.append(labels.numpy())\n",
    "    return np.vstack(predictions), np.vstack(true_labels)\n",
    "\n",
    "# Cross-validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "mse_scores, mae_scores, r2_scores = [], [], []\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    train_dataset = SpectraDataset(X_train, y_train)\n",
    "    test_dataset = SpectraDataset(X_test, y_test)\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    model = Net(input_size=X_train.shape[1])\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    train_model(model, criterion, optimizer, train_loader, num_epochs=250)\n",
    "    predictions, true_labels = evaluate_model(model, test_loader)\n",
    "\n",
    "    mse = mean_squared_error(true_labels, predictions)\n",
    "    mae = mean_absolute_error(true_labels, predictions)\n",
    "    r2 = r2_score(true_labels, predictions)\n",
    "\n",
    "    mse_scores.append(mse)\n",
    "    mae_scores.append(mae)\n",
    "    r2_scores.append(r2)\n",
    "\n",
    "print(f\"Cross-Validated MSE: {np.mean(mse_scores)} ± {np.std(mse_scores)}\")\n",
    "print(f\"Cross-Validated MAE: {np.mean(mae_scores)} ± {np.std(mae_scores)}\")\n",
    "print(f\"Cross-Validated R^2: {np.mean(r2_scores)} ± {np.std(r2_scores)}\")\n",
    "\n",
    "# Feature importance\n",
    "weights = model.fc1.weight.data.numpy()\n",
    "feature_importance = np.sum(np.abs(weights), axis=0)\n",
    "indices = np.argsort(feature_importance)[::-1]\n",
    "\n",
    "# Identify Peaks\n",
    "example_spectrum = np.mean(data_normalized, axis=0)  # Using the mean spectrum for peak detection\n",
    "peaks, properties = find_peaks(example_spectrum, height=0.05, distance=20)  # Adjust height and distance parameters as needed)  # Adjust height parameter as needed\n",
    "\n",
    "print(f\"Detected Peaks: {peaks}\")\n",
    "\n",
    "# Compute Feature Importance for Peaks\n",
    "peak_importances = []\n",
    "\n",
    "for peak in peaks:\n",
    "    # Define a range around the peak (you may need to adjust the range width)\n",
    "    peak_range = range(max(0, peak-2), min(len(feature_importance), peak+3))\n",
    "    \n",
    "    # Compute the importance for the peak as the sum of importances within the peak range\n",
    "    peak_importance = np.sum(feature_importance[list(peak_range)])\n",
    "    peak_importances.append((wavenumbers[0][peak], peak_importance))\n",
    "\n",
    "# Sort peaks by their importance\n",
    "peak_importances.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Extract wavenumbers and importances for plotting\n",
    "if peak_importances:\n",
    "    peak_wavenumbers, peak_importance_values = zip(*peak_importances)\n",
    "\n",
    "    # Plot the peak importances\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    plt.bar(range(len(peak_wavenumbers)), peak_importance_values, align='center')\n",
    "    plt.xticks(range(len(peak_wavenumbers)), peak_wavenumbers, rotation=90)\n",
    "    plt.xlabel('Wavenumber (Peak)')\n",
    "    plt.ylabel('Peak Importance')\n",
    "    plt.title('Peak Importances')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No peaks detected or processed.\")\n",
    "\n",
    "# Final evaluation on a separate test set\n",
    "model.eval()\n",
    "predictions = model(torch.tensor(X_test, dtype=torch.float32)).detach().numpy()\n",
    "r2 = r2_score(y_test, predictions)\n",
    "print(f\"R^2 on test set: {r2}\")\n",
    "\n",
    "# Dictionary containing true values for the unknown samples\n",
    "true_values = {\n",
    "    'test-file-20.dpt': 20.0,  # Replace with actual values\n",
    "    'test-file-66.dpt': 66.6,\n",
    "    'test-file-33.dpt': 33.3,\n",
    "    'test-file-50.dpt': 50.0,\n",
    "    'test-file-51.dpt': 50.0\n",
    "}\n",
    "\n",
    "# Prediction for an unknown sample\n",
    "def predict_concentration_for_unknown_samples(unknown_sample_filename):\n",
    "    print(f'File name: {unknown_sample_filename}')\n",
    "    unknown_sample_filepath = os.path.join('C:\\\\Users\\\\baubl\\\\OneDrive\\\\Stalinis kompiuteris\\\\Nuova cartella\\\\test', unknown_sample_filename)\n",
    "    unknown_spectrum = pd.read_csv(unknown_sample_filepath, delimiter=',', header=None)\n",
    "    index_max_1800 = np.where(unknown_spectrum[0] <= max_wavenumber)[0][-1]\n",
    "    data_trimmed_unknown = unknown_spectrum[1].values[:index_max_1800 + 1]\n",
    "    normalized_unknown_spectrum = data_trimmed_unknown / np.sum(data_trimmed_unknown)\n",
    "    normalized_unknown_spectrum = scaler.transform([normalized_unknown_spectrum])  # Apply same scaling\n",
    "    normalized_unknown_spectrum = torch.tensor(normalized_unknown_spectrum, dtype=torch.float32)\n",
    "\n",
    "    predicted_percentage = model(normalized_unknown_spectrum).item()\n",
    "    print(f'Predicted Percentage: {predicted_percentage}')\n",
    "    \n",
    "    # Calculate MSE and MAE for the prediction\n",
    "    true_value = true_values[unknown_sample_filename]\n",
    "    mse_value = mean_squared_error([true_value], [predicted_percentage])\n",
    "    mae_value = mean_absolute_error([true_value], [predicted_percentage])\n",
    "    print(f'MSE for {unknown_sample_filename}: {mse_value}')\n",
    "    print(f'MAE for {unknown_sample_filename}: {mae_value}')\n",
    "\n",
    "# Predict and print MSE and MAE values for each file\n",
    "predict_concentration_for_unknown_samples('test-file-20.dpt')\n",
    "predict_concentration_for_unknown_samples('test-file-66.dpt')\n",
    "predict_concentration_for_unknown_samples('test-file-33.dpt')\n",
    "predict_concentration_for_unknown_samples('test-file-50.dpt')\n",
    "predict_concentration_for_unknown_samples('test-file-51.dpt')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "raman",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
